{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Model3_Run6_CSDN.ipynb","provenance":[],"authorship_tag":"ABX9TyNrOqLT3VFAy4GIaNw5BHTD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2QDHgQtAN5MG","executionInfo":{"status":"ok","timestamp":1659057961464,"user_tz":-480,"elapsed":22064,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"}},"outputId":"1722db96-55ee-4172-df2b-b2ac7ab8b960"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]},{"output_type":"execute_result","data":{"text/plain":["['entity-aware-relation-classification',\n"," 'Model3_Run3_RE_BiLSTM.ipynb',\n"," 'Model3_Run1.ipynb',\n"," 'FewRel',\n"," 'Model3_Run4_FewShot.ipynb',\n"," 'Model3_Run2_Visualization.ipynb',\n"," 'Model3_Run7_Visualization.ipynb',\n"," 'Model3_Run5_DenpendencePath.ipynb',\n"," 'Model3_Run9.ipynb',\n"," 'exp_data.csv',\n"," 'sentence_1.csv',\n"," 'sentence_2.csv',\n"," 'sentence_3.csv',\n"," 'sentence_4.csv',\n"," 'sentence_5.csv',\n"," 'sentence_6.csv',\n"," 'sentence_7.csv',\n"," 'sentence_8.csv',\n"," 'Model3_Run6_CSDN.ipynb',\n"," 'Model3_Run8_OIE.ipynb',\n"," 'Model3_Run6_Advanced_CSDN.ipynb']"]},"metadata":{},"execution_count":1}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","import os\n","path=\"/content/drive/My Drive/KnowledgeGraph/KG_Constraction/Model3\"\n","\n","os.chdir(path)\n","os.listdir(path)"]},{"cell_type":"code","source":["!pip3 install spacy==2.1.0 # spacy最好使用2.1版本，否则会奇怪地报错\n","!python3 -m spacy download en_core_web_sm # 下载英文\n","!python3 -m spacy download zh_core_web_sm # 下载中文\n","!pip3 install neuralcoref==4.0.0 # 共指消解\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8-b2BdgnN5ix","executionInfo":{"status":"ok","timestamp":1659058037637,"user_tz":-480,"elapsed":76192,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"}},"outputId":"c9486793-1730-4aa6-81a6-51737a3c5b0d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting spacy==2.1.0\n","  Downloading spacy-2.1.0-cp37-cp37m-manylinux1_x86_64.whl (27.7 MB)\n","\u001b[K     |████████████████████████████████| 27.7 MB 43.1 MB/s \n","\u001b[?25hCollecting blis<0.3.0,>=0.2.2\n","  Downloading blis-0.2.4-cp37-cp37m-manylinux1_x86_64.whl (3.2 MB)\n","\u001b[K     |████████████████████████████████| 3.2 MB 49.9 MB/s \n","\u001b[?25hCollecting thinc<7.1.0,>=7.0.2\n","  Downloading thinc-7.0.8-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n","\u001b[K     |████████████████████████████████| 2.1 MB 47.7 MB/s \n","\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.0.7)\n","Collecting jsonschema<3.0.0,>=2.6.0\n","  Downloading jsonschema-2.6.0-py2.py3-none-any.whl (39 kB)\n","Collecting plac<1.0.0,>=0.9.6\n","  Downloading plac-0.9.6-py2.py3-none-any.whl (20 kB)\n","Collecting srsly<1.1.0,>=0.0.5\n","  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n","\u001b[K     |████████████████████████████████| 184 kB 50.8 MB/s \n","\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.0.6)\n","Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (0.9.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.21.6)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.23.0)\n","Collecting preshed<2.1.0,>=2.0.1\n","  Downloading preshed-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (82 kB)\n","\u001b[K     |████████████████████████████████| 82 kB 434 kB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (3.0.4)\n","Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.0) (4.64.0)\n","Installing collected packages: srsly, preshed, plac, blis, thinc, jsonschema, spacy\n","  Attempting uninstall: srsly\n","    Found existing installation: srsly 2.4.4\n","    Uninstalling srsly-2.4.4:\n","      Successfully uninstalled srsly-2.4.4\n","  Attempting uninstall: preshed\n","    Found existing installation: preshed 3.0.6\n","    Uninstalling preshed-3.0.6:\n","      Successfully uninstalled preshed-3.0.6\n","  Attempting uninstall: blis\n","    Found existing installation: blis 0.7.8\n","    Uninstalling blis-0.7.8:\n","      Successfully uninstalled blis-0.7.8\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 8.1.0\n","    Uninstalling thinc-8.1.0:\n","      Successfully uninstalled thinc-8.1.0\n","  Attempting uninstall: jsonschema\n","    Found existing installation: jsonschema 4.3.3\n","    Uninstalling jsonschema-4.3.3:\n","      Successfully uninstalled jsonschema-4.3.3\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 3.4.1\n","    Uninstalling spacy-3.4.1:\n","      Successfully uninstalled spacy-3.4.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","nbclient 0.6.6 requires jupyter-client>=6.1.5, but you have jupyter-client 5.3.5 which is incompatible.\n","nbclient 0.6.6 requires traitlets>=5.2.2, but you have traitlets 5.1.1 which is incompatible.\n","en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.1.0 which is incompatible.\n","altair 4.2.0 requires jsonschema>=3.0, but you have jsonschema 2.6.0 which is incompatible.\u001b[0m\n","Successfully installed blis-0.2.4 jsonschema-2.6.0 plac-0.9.6 preshed-2.0.1 spacy-2.1.0 srsly-1.0.5 thinc-7.0.8\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en_core_web_sm==2.1.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1 MB)\n","\u001b[K     |████████████████████████████████| 11.1 MB 24.6 MB/s \n","\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n","  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.1.0-py3-none-any.whl size=11074433 sha256=15eedcc8c82fe17647cb1c4fa16f74c08f8e2b85eaed4a764a609d88ab139863\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-r3uawwsz/wheels/59/4f/8c/0dbaab09a776d1fa3740e9465078bfd903cc22f3985382b496\n","Successfully built en-core-web-sm\n","Installing collected packages: en-core-web-sm\n","  Attempting uninstall: en-core-web-sm\n","    Found existing installation: en-core-web-sm 3.4.0\n","    Uninstalling en-core-web-sm-3.4.0:\n","      Successfully uninstalled en-core-web-sm-3.4.0\n","Successfully installed en-core-web-sm-2.1.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n","\n","\u001b[38;5;1m✘ No compatible model found for 'zh_core_web_sm' (spaCy v2.1.0).\u001b[0m\n","\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting neuralcoref==4.0.0\n","  Downloading neuralcoref-4.0-cp37-cp37m-manylinux1_x86_64.whl (286 kB)\n","\u001b[K     |████████████████████████████████| 286 kB 32.1 MB/s \n","\u001b[?25hCollecting boto3\n","  Downloading boto3-1.24.40-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 55.1 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0.0) (2.1.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0.0) (2.23.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0.0) (1.21.6)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0.0) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0.0) (1.24.3)\n","Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref==4.0.0) (2.6.0)\n","Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref==4.0.0) (7.0.8)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref==4.0.0) (1.0.7)\n","Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref==4.0.0) (0.9.1)\n","Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref==4.0.0) (1.0.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref==4.0.0) (2.0.6)\n","Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref==4.0.0) (0.9.6)\n","Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref==4.0.0) (2.0.1)\n","Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.1.0->neuralcoref==4.0.0) (0.2.4)\n","Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.1.0,>=7.0.2->spacy>=2.1.0->neuralcoref==4.0.0) (4.64.0)\n","Collecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting botocore<1.28.0,>=1.27.40\n","  Downloading botocore-1.27.40-py3-none-any.whl (9.0 MB)\n","\u001b[K     |████████████████████████████████| 9.0 MB 38.2 MB/s \n","\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 6.8 MB/s \n","\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 55.3 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.40->boto3->neuralcoref==4.0.0) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.40->boto3->neuralcoref==4.0.0) (1.15.0)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, neuralcoref\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed boto3-1.24.40 botocore-1.27.40 jmespath-1.0.1 neuralcoref-4.0 s3transfer-0.6.0 urllib3-1.25.11\n"]}]},{"cell_type":"code","source":["import neuralcoref\n","import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","neuralcoref.add_to_pipe(nlp) # 将共指消解加入spacy的处理流程中\n","text = \"My sister has a dog. She loves him\"\n","doc = nlp(text) # 将text进行pipeline处理\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y57ATE8PN5lK","executionInfo":{"status":"ok","timestamp":1659058049508,"user_tz":-480,"elapsed":6043,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"}},"outputId":"f5e6d987-6051-46fe-fd1e-ac96312c204f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 40155833/40155833 [00:01<00:00, 25640647.27B/s]\n"]}]},{"cell_type":"code","source":["import nltk\n","\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VEXuWHJTP9er","executionInfo":{"status":"ok","timestamp":1659058053911,"user_tz":-480,"elapsed":3178,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"}},"outputId":"641fba46-4678-4d04-f036-d2be21032712"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","import os\n","import pandas as pd\n","import re\n","import spacy\n","from spacy.attrs import intify_attrs\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","import neuralcoref\n","\n","import networkx as nx\n","# import matplotlib.pyplot as plt\n","\n","#nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","all_stop_words = ['many', 'us', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday',\n","                  'today', 'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august',\n","                  'september', 'october', 'november', 'december', 'today', 'old', 'new']\n","all_stop_words = sorted(list(set(all_stop_words + list(stopwords.words('english')))))\n","\n","abspath = os.path.abspath('') ## String which contains absolute path to the script file\n","#print(abspath)\n","os.chdir(abspath)\n","\n","### ==================================================================================================\n","# Tagger\n","\n","def get_tags_spacy(nlp, text):\n","    doc = nlp(text) # 生成词对象\n","    entities_spacy = [] # Entities that Spacy NER found\n","    for ent in doc.ents: # doc.ents表示每个token的实体识别结果\n","        entities_spacy.append([ent.text, ent.start_char, ent.end_char, ent.label_])\n","    return entities_spacy\n","\n","def tag_all(nlp, text, entities_spacy):\n","    if ('neuralcoref' in nlp.pipe_names):\n","        nlp.pipeline.remove('neuralcoref')    \n","    neuralcoref.add_to_pipe(nlp) # Add neural coref to SpaCy's pipe    \n","    doc = nlp(text)\n","    return doc\n","\n","def filter_spans(spans):\n","    # Filter a sequence of spans so they don't contain overlaps\n","    get_sort_key = lambda span: (span.end - span.start, span.start)\n","    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n","    result = []\n","    seen_tokens = set()\n","    for span in sorted_spans:\n","        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n","            result.append(span)\n","            seen_tokens.update(range(span.start, span.end))\n","    return result\n","\n","def tag_chunks(doc):\n","    spans = list(doc.ents) + list(doc.noun_chunks)\n","    spans = filter_spans(spans)\n","    with doc.retokenize() as retokenizer:\n","        string_store = doc.vocab.strings\n","        for span in spans:\n","            start = span.start\n","            end = span.end\n","            retokenizer.merge(doc[start: end], attrs=intify_attrs({'ent_type': 'ENTITY'}, string_store))\n","\n","def tag_chunks_spans(doc, spans, ent_type):\n","    spans = filter_spans(spans)\n","    with doc.retokenize() as retokenizer:\n","        string_store = doc.vocab.strings\n","        for span in spans:\n","            start = span.start\n","            end = span.end\n","            retokenizer.merge(doc[start: end], attrs=intify_attrs({'ent_type': ent_type}, string_store))\n","\n","def clean(text):\n","    # 文本清理\n","    text = text.strip('[(),- :\\'\\\"\\n]\\s*')\n","    text = text.replace('—', ' - ')\n","    text = re.sub('([A-Za-z0-9\\)]{2,}\\.)([A-Z]+[a-z]*)', r\"\\g<1> \\g<2>\", text, flags=re.UNICODE)\n","    text = re.sub('([A-Za-z0-9]{2,}\\.)(\\\"\\w+)', r\"\\g<1> \\g<2>\", text, flags=re.UNICODE)\n","    text = re.sub('([A-Za-z0-9]{2,}\\.\\/)(\\w+)', r\"\\g<1> \\g<2>\", text, flags=re.UNICODE)\n","    text = re.sub('([[A-Z]{1}[[.]{1}[[A-Z]{1}[[.]{1}) ([[A-Z]{1}[a-z]{1,2} )', r\"\\g<1> . \\g<2>\", text, flags=re.UNICODE)\n","    text = re.sub('([A-Za-z]{3,}\\.)([A-Z]+[a-z]+)', r\"\\g<1> \\g<2>\", text, flags=re.UNICODE)\n","    text = re.sub('([[A-Z]{1}[[.]{1}[[A-Z]{1}[[.]{1}) ([[A-Z]{1}[a-z]{1,2} )', r\"\\g<1> . \\g<2>\", text, flags=re.UNICODE)\n","    text = re.sub('([A-Za-z0-9]{2,}\\.)([A-Za-z]+)', r\"\\g<1> \\g<2>\", text, flags=re.UNICODE)\n","    \n","    text = re.sub('’', \"'\", text, flags=re.UNICODE)           # curly apostrophe\n","    text = re.sub('‘', \"'\", text, flags=re.UNICODE)           # curly apostrophe\n","    text = re.sub('“', ' \"', text, flags=re.UNICODE)\n","    text = re.sub('”', ' \"', text, flags=re.UNICODE)\n","    text = re.sub(\"\\|\", \", \", text, flags=re.UNICODE)\n","    text = text.replace('\\t', ' ')\n","    text = re.sub('…', '.', text, flags=re.UNICODE)           # elipsis\n","    text = re.sub('â€¦', '.', text, flags=re.UNICODE)          \n","    text = re.sub('â€“', '-', text)           # long hyphen\n","    text = re.sub('\\s+', ' ', text, flags=re.UNICODE).strip()\n","    text = re.sub(' – ', ' . ', text, flags=re.UNICODE).strip()\n","\n","    return text\n","\n","def tagger(text):  \n","    df_out = pd.DataFrame(columns=['Document#', 'Sentence#', 'Word#', 'Word', 'EntityType', 'EntityIOB', 'Lemma', 'POS', 'POSTag', 'Start', 'End', 'Dependency'])\n","    corefs = [] # 保存所有指代的词\n","    text = clean(text) # 文本清理\n","    \n","    nlp = spacy.load(\"en_core_web_sm\")\n","    entities_spacy = get_tags_spacy(nlp, text) # 获得每个token的实体识别结果\n","    #print(\"SPACY entities:\\n\", ([ent for ent in entities_spacy]), '\\n\\n')\n","    document = tag_all(nlp, text, entities_spacy) # 融入共指消解工具\n","    #for token in document:\n","        #print([token.i, token.text, token.ent_type_, token.ent_iob_, token.lemma_, token.pos_, token.tag_, token.idx, token.idx+len(token)-1, token.dep_])\n","    \n","    ### Coreferences\n","    # \n","    if document._.has_coref:\n","        for cluster in document._.coref_clusters:\n","            main = cluster.main # 共指的词\n","            for m in cluster.mentions: # 所有指代的词（包括其本身）                   \n","                if (str(m).strip() == str(main).strip()): # 如果是其本身，则跳过\n","                    continue\n","                corefs.append([str(m), str(main)]) # 将所有指代的词加入corefs列表\n","    tag_chunks(document)    \n","    \n","    # chunk - somethin OF something 名词分块\n","    spans_change = []\n","    for i in range(2, len(document)):\n","        w_left = document[i-2]\n","        w_middle = document[i-1]\n","        w_right = document[i]\n","        if w_left.dep_ == 'attr':\n","            continue\n","        if w_left.ent_type_ == 'ENTITY' and w_right.ent_type_ == 'ENTITY' and (w_middle.text == 'of'): # or w_middle.text == 'for'): #  or w_middle.text == 'with'\n","            spans_change.append(document[w_left.i : w_right.i + 1])\n","    tag_chunks_spans(document, spans_change, 'ENTITY')\n","    \n","    # chunk verbs with multiple words: 'were exhibited' 动词分块\n","    spans_change_verbs = []\n","    for i in range(1, len(document)):\n","        w_left = document[i-1]\n","        w_right = document[i]\n","        if w_left.pos_ == 'VERB' and (w_right.pos_ == 'VERB'):\n","            spans_change_verbs.append(document[w_left.i : w_right.i + 1])\n","    tag_chunks_spans(document, spans_change_verbs, 'VERB')\n","\n","    # chunk: verb + adp; verb + part \n","    spans_change_verbs = []\n","    for i in range(1, len(document)):\n","        w_left = document[i-1]\n","        w_right = document[i]\n","        if w_left.pos_ == 'VERB' and (w_right.pos_ == 'ADP' or w_right.pos_ == 'PART'):\n","            spans_change_verbs.append(document[w_left.i : w_right.i + 1])\n","    tag_chunks_spans(document, spans_change_verbs, 'VERB')\n","\n","    # chunk: adp + verb; part  + verb\n","    spans_change_verbs = []\n","    for i in range(1, len(document)):\n","        w_left = document[i-1]\n","        w_right = document[i]\n","        if w_right.pos_ == 'VERB' and (w_left.pos_ == 'ADP' or w_left.pos_ == 'PART'):\n","            spans_change_verbs.append(document[w_left.i : w_right.i + 1])\n","    tag_chunks_spans(document, spans_change_verbs, 'VERB')\n","    \n","    # chunk verbs with multiple words: 'were exhibited'\n","    spans_change_verbs = []\n","    for i in range(1, len(document)):\n","        w_left = document[i-1]\n","        w_right = document[i]\n","        if w_left.pos_ == 'VERB' and (w_right.pos_ == 'VERB'):\n","            spans_change_verbs.append(document[w_left.i : w_right.i + 1])\n","    tag_chunks_spans(document, spans_change_verbs, 'VERB')\n","\n","    # chunk all between LRB- -RRB- (something between brackets)\n","    start = 0\n","    end = 0\n","    spans_between_brackets = []\n","    for i in range(0, len(document)):\n","        if ('-LRB-' == document[i].tag_ or r\"(\" in document[i].text):\n","            start = document[i].i\n","            continue\n","        if ('-RRB-' == document[i].tag_ or r')' in document[i].text):\n","            end = document[i].i + 1\n","        if (end > start and not start == 0):\n","            span = document[start:end]\n","            try:\n","                assert (u\"(\" in span.text and u\")\" in span.text)\n","            except:\n","                pass\n","                #print(span)\n","            spans_between_brackets.append(span)\n","            start = 0\n","            end = 0\n","    tag_chunks_spans(document, spans_between_brackets, 'ENTITY')\n","            \n","    # chunk entities  两个实体相邻时，合并\n","    spans_change_verbs = []\n","    for i in range(1, len(document)):\n","        w_left = document[i-1]\n","        w_right = document[i]\n","        if w_left.ent_type_ == 'ENTITY' and w_right.ent_type_ == 'ENTITY':\n","            spans_change_verbs.append(document[w_left.i : w_right.i + 1])\n","    tag_chunks_spans(document, spans_change_verbs, 'ENTITY')\n","    \n","    doc_id = 1\n","    count_sentences = 0\n","    prev_dep = 'nsubj'\n","    for token in document:\n","        if (token.dep_ == 'ROOT'):\n","            if token.pos_ == 'VERB':\n","                #  将pipeline的输出保存到csv，列名：['Document#', 'Sentence#', 'Word#', 'Word', 'EntityType', 'EntityIOB', 'Lemma', 'POS', 'POSTag', 'Start', 'End', 'Dependency']\n","                df_out.loc[len(df_out)] = [doc_id, count_sentences, token.i, token.text, token.ent_type_, token.ent_iob_, token.lemma_, token.pos_, token.tag_, token.idx, token.idx+len(token)-1, token.dep_]\n","            else:\n","                df_out.loc[len(df_out)] = [doc_id, count_sentences, token.i, token.text, token.ent_type_, token.ent_iob_, token.lemma_, token.pos_, token.tag_, token.idx, token.idx+len(token)-1, prev_dep]\n","        else:\n","            df_out.loc[len(df_out)] = [doc_id, count_sentences, token.i, token.text, token.ent_type_, token.ent_iob_, token.lemma_, token.pos_, token.tag_, token.idx, token.idx+len(token)-1, token.dep_]\n","                  \n","        if (token.text == '.'):\n","            count_sentences += 1\n","        prev_dep = token.dep_\n","        \n","    return df_out, corefs\n","\n","### ==================================================================================================\n","### triple extractor\n","\n","def get_predicate(s):\n","    pred_ids = {}\n","    for w, index, spo in s:\n","        if spo == 'predicate' and w != \"'s\" and w != \"\\\"\": #= 11.95\n","            pred_ids[index] = w\n","    predicates = {}\n","    for key, value in pred_ids.items():\n","        predicates[key] = value\n","    return predicates\n","\n","def get_subjects(s, start, end, adps):\n","    subjects = {}\n","    for w, index, spo in s:\n","        if index >= start and index <= end:\n","            if 'subject' in spo or 'entity' in spo or 'object' in spo:\n","                subjects[index] = w\n","    return subjects\n","    \n","def get_objects(s, start, end, adps):\n","    objects = {}\n","    for w, index, spo in s:\n","        if index >= start and index <= end:\n","            if 'object' in spo or 'entity' in spo or 'subject' in spo:\n","                objects[index] = w\n","    return objects\n","\n","def get_positions(s, start, end):\n","    adps = {}\n","    for w, index, spo in s:        \n","        if index >= start and index <= end:\n","            if 'of' == spo or 'at' == spo:\n","                adps[index] = w\n","    return adps\n","\n","def create_triples(df_text, corefs):\n","    ## 创建三元组\n","    sentences = [] # 所有句子\n","    aSentence = [] # 某个句子\n","    \n","    for index, row in df_text.iterrows():  # 给每个词块打上标签 主要为四类\n","        d_id, s_id, word_id, word, ent, ent_iob, lemma, cg_pos, pos, start, end, dep = row.items()\n","        if 'subj' in dep[1]:\n","            aSentence.append([word[1], word_id[1], 'subject'])\n","        elif 'ROOT' in dep[1] or 'VERB' in cg_pos[1] or pos[1] == 'IN':\n","            aSentence.append([word[1], word_id[1], 'predicate'])\n","        elif 'obj' in dep[1]:\n","            aSentence.append([word[1], word_id[1], 'object'])\n","        elif ent[1] == 'ENTITY':\n","            aSentence.append([word[1], word_id[1], 'entity'])        \n","        elif word[1] == '.':\n","            sentences.append(aSentence)\n","            aSentence = []\n","        else:\n","            aSentence.append([word[1], word_id[1], pos[1]])\n","    \n","    relations = []\n","    #loose_entities = []\n","    for s in sentences:\n","        if len(s) == 0: continue\n","        preds = get_predicate(s) # 拿到一个句子里的所有标签为动词的单词块\n","        \"\"\"\n","        if preds == {}: \n","            preds = {p[1]:p[0] for p in s if (p[2] == 'JJ' or p[2] == 'IN' or p[2] == 'CC' or\n","                     p[2] == 'RP' or p[2] == ':' or p[2] == 'predicate' or\n","                     p[2] =='-LRB-' or p[2] =='-RRB-') }\n","            if preds == {}:\n","                #print('\\npred = 0', s)\n","                preds = {p[1]:p[0] for p in s if (p[2] == ',')}\n","                if preds == {}:\n","                    ents = [e[0] for e in s if e[2] == 'entity']\n","                    if (ents):\n","                        loose_entities = ents # not significant for now\n","                        #print(\"Loose entities = \", ents)\n","        \"\"\"\n","        if preds:\n","            if (len(preds) == 1):\n","                #print(\"preds = \", preds)\n","                predicate = list(preds.values())[0]\n","                if (len(predicate) < 2):\n","                    predicate = 'is'\n","                #print(s)\n","                ents = [e[0] for e in s if e[2] == 'entity']\n","                #print('ents = ', ents)\n","                for i in range(1, len(ents)):\n","                    relations.append([ents[0], predicate, ents[i]])\n","\n","            pred_ids = list(preds.keys())\n","            pred_ids.append(s[0][1])\n","            pred_ids.append(s[len(s)-1][1])\n","            pred_ids.sort()\n","                    \n","            for i in range(1, len(pred_ids)-1):\n","                predicate = preds[pred_ids[i]]\n","                adps_subjs = get_positions(s, pred_ids[i-1], pred_ids[i]) \n","                #print(adps_subjs)\n","                subjs = get_subjects(s, pred_ids[i-1], pred_ids[i], adps_subjs)  # 在两个predicate之间找后一个动词的主语\n","                adps_objs = get_positions(s, pred_ids[i], pred_ids[i+1])\n","                objs = get_objects(s, pred_ids[i], pred_ids[i+1], adps_objs) # 在两个predicate之间找前一个动词的宾语\n","                for k_s, subj in subjs.items():                \n","                    for k_o, obj in objs.items():\n","                        obj_prev_id = int(k_o) - 1\n","                        if obj_prev_id in adps_objs: # at, in, of\n","                            relations.append([subj, predicate + ' ' + adps_objs[obj_prev_id], obj])\n","                        else:\n","                            relations.append([subj, predicate, obj])\n","                print(predicate,relations)\n","    \n","    ### Read coreferences: coreference files are TAB separated values\n","    coreferences = []\n","    for val in corefs:\n","        if val[0].strip() != val[1].strip():\n","            if len(val[0]) <= 50 and len(val[1]) <= 50:\n","                co_word = val[0]\n","                real_word = val[1].strip('[,- \\'\\n]*')\n","                real_word = re.sub(\"'s$\", '', real_word, flags=re.UNICODE)\n","                if (co_word != real_word):\n","                    coreferences.append([co_word, real_word])\n","            else:\n","                co_word = val[0]\n","                real_word = ' '.join((val[1].strip('[,- \\'\\n]*')).split()[:7])\n","                real_word = re.sub(\"'s$\", '', real_word, flags=re.UNICODE)\n","                if (co_word != real_word):\n","                    coreferences.append([co_word, real_word])\n","                \n","    # Resolve corefs\n","    triples_object_coref_resolved = []\n","    triples_all_coref_resolved = []\n","    for s, p, o in relations:\n","        coref_resolved = False\n","        for co in coreferences:\n","            if (s == co[0]):\n","                subj = co[1]\n","                triples_object_coref_resolved.append([subj, p, o])\n","                coref_resolved = True\n","                break\n","        if not coref_resolved:\n","            triples_object_coref_resolved.append([s, p, o])\n","\n","    for s, p, o in triples_object_coref_resolved:\n","        coref_resolved = False\n","        for co in coreferences:\n","            if (o == co[0]):\n","                obj = co[1]\n","                triples_all_coref_resolved.append([s, p, obj])\n","                coref_resolved = True\n","                break\n","        if not coref_resolved:\n","            triples_all_coref_resolved.append([s, p, o])\n","    return(triples_all_coref_resolved)\n","\n","def get_graph(triples):\n","    G = nx.DiGraph()\n","    for s, p, o in triples:\n","        G.add_edge(s, o, key=p)\n","    return G\n","\n","def get_entities_with_capitals(G):\n","    entities = []\n","    for node in G.nodes():\n","        if (any(ch.isupper() for ch in list(node))):\n","            entities.append(node)\n","    return entities\n","\n","def get_paths_between_capitalised_entities(triples):\n","    \n","    g = get_graph(triples)\n","    ents_capitals = get_entities_with_capitals(g)\n","    print(ents_capitals)\n","    paths = []\n","    #print('\\nShortest paths among capitalised words -------------------')\n","    for i in range(0, len(ents_capitals)):\n","        n1 = ents_capitals[i]\n","        for j in range(1, len(ents_capitals)):\n","            try:\n","                n2 = ents_capitals[j]\n","                path = nx.shortest_path(g, source=n1, target=n2)\n","                if path and len(path) > 2:\n","                    paths.append(path)\n","                path = nx.shortest_path(g, source=n2, target=n1)\n","                if path and len(path) > 2:\n","                    paths.append(path)\n","            except Exception:\n","                continue\n","    return g, paths\n","\n","\n","def get_paths(doc_triples):\n","    triples = []\n","    g, paths = get_paths_between_capitalised_entities(doc_triples)\n","    for p in paths:\n","        path = [(u, g[u][v]['key'], v) for (u, v) in zip(p[0:], p[1:])]\n","        length = len(p)\n","        if (path[length-2][1] == 'in' or path[length-2][1] == 'at' or path[length-2][1] == 'on'):\n","            if [path[0][0], path[length-2][1], path[length-2][2]] not in triples:\n","                triples.append([path[0][0], path[length-2][1], path[length-2][2]])\n","        elif (' in' in path[length-2][1] or ' at' in path[length-2][1] or ' on' in path[length-2][1]):\n","            if [path[0][0], path[length-2][1], path[length-2][2]] not in triples:\n","                triples.append([path[0][0], 'in', path[length-2][2]])\n","    for t in doc_triples:\n","        if t not in triples:\n","            triples.append(t)\n","    return triples\n","\n","\n","# 抽取三元组\n","def extract_triples(text):\n","    df_tagged, corefs = tagger(text) # pipeline处理文本，并返回每个token的特征，以及共指消解的结果\n","    doc_triples = create_triples(df_tagged, corefs)\n","    all_triples = get_paths(doc_triples)\n","    filtered_triples = []    \n","    for s, p, o in all_triples:\n","        if ([s, p, o] not in filtered_triples):\n","            if s.lower() in all_stop_words or o.lower() in all_stop_words:\n","                continue\n","            elif s == p:\n","                continue\n","            if s.isdigit() or o.isdigit():\n","                continue\n","            if '%' in o or '%' in s: #= 11.96\n","                continue\n","            if (len(s) < 2) or (len(o) < 2):\n","                continue\n","            if (s.islower() and len(s) < 4) or (o.islower() and len(o) < 4):\n","                continue\n","            if s == o:\n","                continue            \n","            subj = s.strip('[,- :\\'\\\"\\n]*')\n","            pred = p.strip('[- :\\'\\\"\\n]*.')\n","            obj = o.strip('[,- :\\'\\\"\\n]*')\n","            \n","            for sw in ['a', 'an', 'the', 'its', 'their', 'his', 'her', 'our', 'all', 'old', 'new', 'latest', 'who', 'that', 'this', 'these', 'those']:\n","                subj = ' '.join(word for word in subj.split() if not word == sw)\n","                obj = ' '.join(word for word in obj.split()  if not word == sw)\n","            subj = re.sub(\"\\s\\s+\", \" \", subj)\n","            obj = re.sub(\"\\s\\s+\", \" \", obj)\n","            \n","            if subj and pred and obj:\n","                filtered_triples.append([subj, pred, obj])\n","\n","    #TRIPLES = rank_by_degree(filtered_triples)\n","    return filtered_triples"],"metadata":{"id":"9asVVeQaN5p1","executionInfo":{"status":"ok","timestamp":1659058060376,"user_tz":-480,"elapsed":6482,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["sentence_1 = 'with the significate progress in natural language processing and computer vision, extracting valuable information from a lot of bio medical literature has gained great popularity among researchers ,and deep learning has boosted the development of effective biomedical text mining models.'\n","sentence_2 = 'While BERT obtains performance comparable to that of previous models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition, biomedical relation extraction and biomedical question answering.'\n","sentence_3 = 'We view this problem as a link prediction task in a knowledge graph, and show that a new model of the top discriminative meta paths is able to understand the meaning of some statement and accurately determine its veracity.'\n","sentence_4 = 'the story is that cui win the game.'\n","sentence_5 = 'We evaluate our approach by examining thousands of claims related to history, geography, biology, and politics using a public million node knowledge graph extracted from Wikipedia and PubMedDB.'\n","sentence_6 = 'Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows.'  # 这种没有主谓宾的结构是抽不出来的\n","sentence_7 = 'However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora.'\n","sentence_8 = 'With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora.'\n","\n","df_tagged, corefs = tagger(sentence_8)\n","df_tagged.to_csv(\"sentence_8.csv\",index=False)\n","df_tagged"],"metadata":{"id":"sug7Q0rMqUyl","executionInfo":{"status":"ok","timestamp":1659058061894,"user_tz":-480,"elapsed":1532,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"}},"colab":{"base_uri":"https://localhost:8080/","height":763},"outputId":"d6904940-04ad-44f7-ce16-386f507d0538"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: FutureWarning: Possible nested set at position 2\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: FutureWarning: Possible nested set at position 11\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: FutureWarning: Possible nested set at position 18\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: FutureWarning: Possible nested set at position 27\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: FutureWarning: Possible nested set at position 37\n"]},{"output_type":"execute_result","data":{"text/plain":["   Document# Sentence# Word#                                       Word  \\\n","0          1         0     0                                       With   \n","1          1         0     1               almost the same architecture   \n","2          1         0     2                                     across   \n","3          1         0     3                                      tasks   \n","4          1         0     4                                          ,   \n","5          1         0     5                                    BioBERT   \n","6          1         0     6                                    largely   \n","7          1         0     7                                outperforms   \n","8          1         0     8                                       BERT   \n","9          1         0     9                                        and   \n","10         1         0    10                            previous models   \n","11         1         0    11                                         in   \n","12         1         0    12  a variety of biomedical text mining tasks   \n","13         1         0    13                                       when   \n","14         1         0    14                                        pre   \n","15         1         0    15                                          -   \n","16         1         0    16                                    trained   \n","17         1         0    17                                         on   \n","18         1         0    18                         biomedical corpora   \n","19         1         0    19                                          .   \n","\n","   EntityType EntityIOB       Lemma    POS POSTag Start  End Dependency  \n","0                     O        with    ADP     IN     0    3       prep  \n","1      ENTITY         O      almost    ADV     RB     5   32       pobj  \n","2                     O      across    ADP     IN    34   39       prep  \n","3      ENTITY         O        task   NOUN    NNS    41   45       pobj  \n","4                     O           ,  PUNCT      ,    46   46      punct  \n","5      ENTITY         O     BioBERT   NOUN    NNS    48   54      nsubj  \n","6                     O     largely    ADV     RB    56   62     advmod  \n","7                     O  outperform   VERB    VBZ    64   74       ROOT  \n","8                     O        bert    ADJ     JJ    76   79       dobj  \n","9                     O         and  CCONJ     CC    81   83         cc  \n","10     ENTITY         O    previous    ADJ     JJ    85   99       conj  \n","11                    O          in    ADP     IN   101  102       prep  \n","12     ENTITY         O           a    DET     DT   104  144       pobj  \n","13                    O        when    ADV    WRB   146  149     advmod  \n","14                    O         pre    ADJ     JJ   151  153     subtok  \n","15                    O           -   VERB    VBN   154  154     subtok  \n","16                    O     trained    ADJ     JJ   155  161      advcl  \n","17                    O          on    ADP     IN   163  164       prep  \n","18     ENTITY         O  biomedical    ADJ     JJ   166  183       pobj  \n","19                    O           .  PUNCT      .   184  184      punct  "],"text/html":["\n","  <div id=\"df-8a7f5604-0a50-40df-8b86-8c30cca21e1e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Document#</th>\n","      <th>Sentence#</th>\n","      <th>Word#</th>\n","      <th>Word</th>\n","      <th>EntityType</th>\n","      <th>EntityIOB</th>\n","      <th>Lemma</th>\n","      <th>POS</th>\n","      <th>POSTag</th>\n","      <th>Start</th>\n","      <th>End</th>\n","      <th>Dependency</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>With</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>with</td>\n","      <td>ADP</td>\n","      <td>IN</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>prep</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>almost the same architecture</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>almost</td>\n","      <td>ADV</td>\n","      <td>RB</td>\n","      <td>5</td>\n","      <td>32</td>\n","      <td>pobj</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>across</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>across</td>\n","      <td>ADP</td>\n","      <td>IN</td>\n","      <td>34</td>\n","      <td>39</td>\n","      <td>prep</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>tasks</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>task</td>\n","      <td>NOUN</td>\n","      <td>NNS</td>\n","      <td>41</td>\n","      <td>45</td>\n","      <td>pobj</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>,</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>,</td>\n","      <td>PUNCT</td>\n","      <td>,</td>\n","      <td>46</td>\n","      <td>46</td>\n","      <td>punct</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>BioBERT</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>BioBERT</td>\n","      <td>NOUN</td>\n","      <td>NNS</td>\n","      <td>48</td>\n","      <td>54</td>\n","      <td>nsubj</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>largely</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>largely</td>\n","      <td>ADV</td>\n","      <td>RB</td>\n","      <td>56</td>\n","      <td>62</td>\n","      <td>advmod</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>outperforms</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>outperform</td>\n","      <td>VERB</td>\n","      <td>VBZ</td>\n","      <td>64</td>\n","      <td>74</td>\n","      <td>ROOT</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>BERT</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>bert</td>\n","      <td>ADJ</td>\n","      <td>JJ</td>\n","      <td>76</td>\n","      <td>79</td>\n","      <td>dobj</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>and</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>and</td>\n","      <td>CCONJ</td>\n","      <td>CC</td>\n","      <td>81</td>\n","      <td>83</td>\n","      <td>cc</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>previous models</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>previous</td>\n","      <td>ADJ</td>\n","      <td>JJ</td>\n","      <td>85</td>\n","      <td>99</td>\n","      <td>conj</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>11</td>\n","      <td>in</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>in</td>\n","      <td>ADP</td>\n","      <td>IN</td>\n","      <td>101</td>\n","      <td>102</td>\n","      <td>prep</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>12</td>\n","      <td>a variety of biomedical text mining tasks</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>a</td>\n","      <td>DET</td>\n","      <td>DT</td>\n","      <td>104</td>\n","      <td>144</td>\n","      <td>pobj</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>when</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>when</td>\n","      <td>ADV</td>\n","      <td>WRB</td>\n","      <td>146</td>\n","      <td>149</td>\n","      <td>advmod</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>14</td>\n","      <td>pre</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>pre</td>\n","      <td>ADJ</td>\n","      <td>JJ</td>\n","      <td>151</td>\n","      <td>153</td>\n","      <td>subtok</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>15</td>\n","      <td>-</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>-</td>\n","      <td>VERB</td>\n","      <td>VBN</td>\n","      <td>154</td>\n","      <td>154</td>\n","      <td>subtok</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>trained</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>trained</td>\n","      <td>ADJ</td>\n","      <td>JJ</td>\n","      <td>155</td>\n","      <td>161</td>\n","      <td>advcl</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>17</td>\n","      <td>on</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>on</td>\n","      <td>ADP</td>\n","      <td>IN</td>\n","      <td>163</td>\n","      <td>164</td>\n","      <td>prep</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>18</td>\n","      <td>biomedical corpora</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>biomedical</td>\n","      <td>ADJ</td>\n","      <td>JJ</td>\n","      <td>166</td>\n","      <td>183</td>\n","      <td>pobj</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>19</td>\n","      <td>.</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>.</td>\n","      <td>PUNCT</td>\n","      <td>.</td>\n","      <td>184</td>\n","      <td>184</td>\n","      <td>punct</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a7f5604-0a50-40df-8b86-8c30cca21e1e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8a7f5604-0a50-40df-8b86-8c30cca21e1e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8a7f5604-0a50-40df-8b86-8c30cca21e1e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["sentence_9 = 'We expand the item response theory to study the case of cheating students for a set of exams , trying to detect cheating students by applying a greedy algorithm of inference . a greedy algorithm of inference is closely related to the Boltzmann machine learning .'\n","df_tagged, corefs = tagger(sentence_9)\n","df_tagged.to_csv(\"sentence_9.csv\",index=False)\n","df_tagged"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":708},"id":"aT6DfmZblZEG","executionInfo":{"status":"ok","timestamp":1659058711149,"user_tz":-480,"elapsed":963,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"}},"outputId":"29ec751d-07d5-4474-dedc-566740d898cb"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Document# Sentence# Word#                                  Word EntityType  \\\n","0          1         0     0                                    We     ENTITY   \n","1          1         0     1                                expand              \n","2          1         0     2              the item response theory     ENTITY   \n","3          1         0     3                              to study       VERB   \n","4          1         0     4                              the case     ENTITY   \n","5          1         0     5                           of cheating       VERB   \n","6          1         0     6                              students     ENTITY   \n","7          1         0     7                                   for              \n","8          1         0     8                        a set of exams     ENTITY   \n","9          1         0     9                                     ,              \n","10         1         0    10                             trying to       VERB   \n","11         1         0    11  detect cheating students by applying       VERB   \n","12         1         0    12       a greedy algorithm of inference     ENTITY   \n","13         1         0    13                                     .              \n","14         1         1    14       a greedy algorithm of inference     ENTITY   \n","15         1         1    15                                    is              \n","16         1         1    16                               closely              \n","17         1         1    17                               related              \n","18         1         1    18                                    to              \n","19         1         1    19        the Boltzmann machine learning     ENTITY   \n","20         1         1    20                                     .              \n","\n","   EntityIOB    Lemma    POS POSTag Start  End Dependency  \n","0          O   -PRON-   PRON    PRP     0    1      nsubj  \n","1          O   expand   VERB    VBP     3    8       ROOT  \n","2          O      the    DET     DT    10   33       dobj  \n","3          O       to   PART     TO    35   42      advcl  \n","4          O      the    DET     DT    44   51       dobj  \n","5          O       of    ADP     IN    53   63       prep  \n","6          O  student   NOUN    NNS    65   72       dobj  \n","7          O      for    ADP     IN    74   76       prep  \n","8          O        a    DET     DT    78   91       pobj  \n","9          O        ,  PUNCT      ,    93   93      punct  \n","10         O      try   VERB    VBG    95  103      advcl  \n","11         O   detect   VERB     VB   105  140      xcomp  \n","12         O        a    DET     DT   142  172       dobj  \n","13         O        .  PUNCT      .   174  174      punct  \n","14         O        a    DET     DT   176  206  nsubjpass  \n","15         O       be   VERB    VBZ   208  209    auxpass  \n","16         O  closely    ADV     RB   211  217     advmod  \n","17         O  related    ADJ     JJ   219  225     advmod  \n","18         O       to    ADP     IN   227  228       prep  \n","19         O      the    DET     DT   230  259       pobj  \n","20         O        .  PUNCT      .   261  261      punct  "],"text/html":["\n","  <div id=\"df-79794598-4411-412d-b237-f65ce9897734\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Document#</th>\n","      <th>Sentence#</th>\n","      <th>Word#</th>\n","      <th>Word</th>\n","      <th>EntityType</th>\n","      <th>EntityIOB</th>\n","      <th>Lemma</th>\n","      <th>POS</th>\n","      <th>POSTag</th>\n","      <th>Start</th>\n","      <th>End</th>\n","      <th>Dependency</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>We</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>-PRON-</td>\n","      <td>PRON</td>\n","      <td>PRP</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>nsubj</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>expand</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>expand</td>\n","      <td>VERB</td>\n","      <td>VBP</td>\n","      <td>3</td>\n","      <td>8</td>\n","      <td>ROOT</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>the item response theory</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>the</td>\n","      <td>DET</td>\n","      <td>DT</td>\n","      <td>10</td>\n","      <td>33</td>\n","      <td>dobj</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>to study</td>\n","      <td>VERB</td>\n","      <td>O</td>\n","      <td>to</td>\n","      <td>PART</td>\n","      <td>TO</td>\n","      <td>35</td>\n","      <td>42</td>\n","      <td>advcl</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>the case</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>the</td>\n","      <td>DET</td>\n","      <td>DT</td>\n","      <td>44</td>\n","      <td>51</td>\n","      <td>dobj</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>of cheating</td>\n","      <td>VERB</td>\n","      <td>O</td>\n","      <td>of</td>\n","      <td>ADP</td>\n","      <td>IN</td>\n","      <td>53</td>\n","      <td>63</td>\n","      <td>prep</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>students</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>student</td>\n","      <td>NOUN</td>\n","      <td>NNS</td>\n","      <td>65</td>\n","      <td>72</td>\n","      <td>dobj</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>for</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>for</td>\n","      <td>ADP</td>\n","      <td>IN</td>\n","      <td>74</td>\n","      <td>76</td>\n","      <td>prep</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>a set of exams</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>a</td>\n","      <td>DET</td>\n","      <td>DT</td>\n","      <td>78</td>\n","      <td>91</td>\n","      <td>pobj</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>,</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>,</td>\n","      <td>PUNCT</td>\n","      <td>,</td>\n","      <td>93</td>\n","      <td>93</td>\n","      <td>punct</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>trying to</td>\n","      <td>VERB</td>\n","      <td>O</td>\n","      <td>try</td>\n","      <td>VERB</td>\n","      <td>VBG</td>\n","      <td>95</td>\n","      <td>103</td>\n","      <td>advcl</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>11</td>\n","      <td>detect cheating students by applying</td>\n","      <td>VERB</td>\n","      <td>O</td>\n","      <td>detect</td>\n","      <td>VERB</td>\n","      <td>VB</td>\n","      <td>105</td>\n","      <td>140</td>\n","      <td>xcomp</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>12</td>\n","      <td>a greedy algorithm of inference</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>a</td>\n","      <td>DET</td>\n","      <td>DT</td>\n","      <td>142</td>\n","      <td>172</td>\n","      <td>dobj</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>.</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>.</td>\n","      <td>PUNCT</td>\n","      <td>.</td>\n","      <td>174</td>\n","      <td>174</td>\n","      <td>punct</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>14</td>\n","      <td>a greedy algorithm of inference</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>a</td>\n","      <td>DET</td>\n","      <td>DT</td>\n","      <td>176</td>\n","      <td>206</td>\n","      <td>nsubjpass</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>15</td>\n","      <td>is</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>be</td>\n","      <td>VERB</td>\n","      <td>VBZ</td>\n","      <td>208</td>\n","      <td>209</td>\n","      <td>auxpass</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>16</td>\n","      <td>closely</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>closely</td>\n","      <td>ADV</td>\n","      <td>RB</td>\n","      <td>211</td>\n","      <td>217</td>\n","      <td>advmod</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>17</td>\n","      <td>related</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>related</td>\n","      <td>ADJ</td>\n","      <td>JJ</td>\n","      <td>219</td>\n","      <td>225</td>\n","      <td>advmod</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>18</td>\n","      <td>to</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>to</td>\n","      <td>ADP</td>\n","      <td>IN</td>\n","      <td>227</td>\n","      <td>228</td>\n","      <td>prep</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>19</td>\n","      <td>the Boltzmann machine learning</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>the</td>\n","      <td>DET</td>\n","      <td>DT</td>\n","      <td>230</td>\n","      <td>259</td>\n","      <td>pobj</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>20</td>\n","      <td>.</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>.</td>\n","      <td>PUNCT</td>\n","      <td>.</td>\n","      <td>261</td>\n","      <td>261</td>\n","      <td>punct</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79794598-4411-412d-b237-f65ce9897734')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-79794598-4411-412d-b237-f65ce9897734 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-79794598-4411-412d-b237-f65ce9897734');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":[""],"metadata":{"id":"4kHy62zClZKR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from spacy import displacy\n","doc = nlp(sentence_1)\n","displacy.render(doc, style='dep', jupyter=True,options = {'distance': 120})"],"metadata":{"id":"XNdPgoEeN5u5","executionInfo":{"status":"error","timestamp":1658825349346,"user_tz":-480,"elapsed":10341,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"}},"colab":{"base_uri":"https://localhost:8080/","height":201},"outputId":"2de6ee68-174c-40e5-cef6-4a4c10f25bac"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-0b511c61a633>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdisplacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dep'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjupyter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"]}]},{"cell_type":"code","source":["df_tagged, corefs = tagger(text)\n","doc_triples = create_triples(df_tagged, corefs)\n","all_triples = get_paths(doc_triples)"],"metadata":{"id":"QiwQlpAQN55V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656300360871,"user_tz":-480,"elapsed":780,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"}},"outputId":"7ae50439-0a57-4978-d896-8b1a57271154"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["compare [['We', 'compare', 'a standard approach'], ['We', 'compare', 'the sparse interactions']]\n","in [['We', 'compare', 'a standard approach'], ['We', 'compare', 'the sparse interactions'], ['a standard approach', 'in', 'the Boltzmann machine'], ['the sparse interactions', 'in', 'the Boltzmann machine']]\n","learning to [['We', 'compare', 'a standard approach'], ['We', 'compare', 'the sparse interactions'], ['a standard approach', 'in', 'the Boltzmann machine'], ['the sparse interactions', 'in', 'the Boltzmann machine'], ['the Boltzmann machine', 'learning to', 'our greedy algorithm'], ['the Boltzmann machine', 'learning to', 'we']]\n","find [['We', 'compare', 'a standard approach'], ['We', 'compare', 'the sparse interactions'], ['a standard approach', 'in', 'the Boltzmann machine'], ['the sparse interactions', 'in', 'the Boltzmann machine'], ['the Boltzmann machine', 'learning to', 'our greedy algorithm'], ['the Boltzmann machine', 'learning to', 'we'], ['our greedy algorithm', 'find', 'a greedy algorithm of inference'], ['we', 'find', 'a greedy algorithm of inference']]\n","in [['We', 'compare', 'a standard approach'], ['We', 'compare', 'the sparse interactions'], ['a standard approach', 'in', 'the Boltzmann machine'], ['the sparse interactions', 'in', 'the Boltzmann machine'], ['the Boltzmann machine', 'learning to', 'our greedy algorithm'], ['the Boltzmann machine', 'learning to', 'we'], ['our greedy algorithm', 'find', 'a greedy algorithm of inference'], ['we', 'find', 'a greedy algorithm of inference'], ['a greedy algorithm of inference', 'in', 'several aspects']]\n","['We', 'the Boltzmann machine']\n"]}]},{"cell_type":"code","source":["doc_triples"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AwI3OpWULbW-","executionInfo":{"status":"ok","timestamp":1656300101713,"user_tz":-480,"elapsed":10,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"}},"outputId":"831ae3a6-de02-45d1-be56-bed164ad065e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['We', 'compare', 'a standard approach'],\n"," ['We', 'compare', 'the sparse interactions'],\n"," ['a standard approach', 'in', 'the Boltzmann machine'],\n"," ['the sparse interactions', 'in', 'the Boltzmann machine'],\n"," ['the Boltzmann machine', 'learning to', 'our greedy algorithm'],\n"," ['the Boltzmann machine', 'learning to', 'we'],\n"," ['our greedy algorithm', 'find', 'a greedy algorithm of inference'],\n"," ['we', 'find', 'a greedy algorithm of inference'],\n"," ['a greedy algorithm of inference', 'in', 'several aspects']]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["all_triples"],"metadata":{"id":"sUDOXKwphYIY","executionInfo":{"status":"ok","timestamp":1656300124373,"user_tz":-480,"elapsed":446,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"}},"outputId":"ee0b5374-c661-4aa3-d9b9-115b4326e51a","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['We', 'in', 'the Boltzmann machine'],\n"," ['We', 'compare', 'a standard approach'],\n"," ['We', 'compare', 'the sparse interactions'],\n"," ['a standard approach', 'in', 'the Boltzmann machine'],\n"," ['the sparse interactions', 'in', 'the Boltzmann machine'],\n"," ['the Boltzmann machine', 'learning to', 'our greedy algorithm'],\n"," ['the Boltzmann machine', 'learning to', 'we'],\n"," ['our greedy algorithm', 'find', 'a greedy algorithm of inference'],\n"," ['we', 'find', 'a greedy algorithm of inference'],\n"," ['a greedy algorithm of inference', 'in', 'several aspects']]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["df_tagged"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":614},"id":"AO6518ZqRH9J","executionInfo":{"status":"ok","timestamp":1656251551859,"user_tz":-480,"elapsed":18,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"}},"outputId":"2810b71e-19d2-4f27-e86b-42255dfb0a31"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Document# Sentence# Word#                             Word EntityType  \\\n","0          1         0     0                               We     ENTITY   \n","1          1         0     1                          compare              \n","2          1         0     2              a standard approach     ENTITY   \n","3          1         0     3                         to infer       VERB   \n","4          1         0     4          the sparse interactions     ENTITY   \n","5          1         0     5                               in              \n","6          1         0     6            the Boltzmann machine     ENTITY   \n","7          1         0     7                      learning to       VERB   \n","8          1         0     8             our greedy algorithm     ENTITY   \n","9          1         0     9                              and              \n","10         1         0    10                               we     ENTITY   \n","11         1         0    11                             find              \n","12         1         0    12  a greedy algorithm of inference     ENTITY   \n","13         1         0    13                            to be       VERB   \n","14         1         0    14                         superior              \n","15         1         0    15                               in              \n","16         1         0    16                  several aspects     ENTITY   \n","17         1         0    17                                .              \n","\n","   EntityIOB     Lemma    POS POSTag Start  End Dependency  \n","0          O    -PRON-   PRON    PRP     0    1      nsubj  \n","1          O   compare   VERB    VBP     3    9       ROOT  \n","2          O         a    DET     DT    11   29       dobj  \n","3          O        to   PART     TO    31   38      advcl  \n","4          O       the    DET     DT    40   62       dobj  \n","5          O        in    ADP     IN    64   65       prep  \n","6          O       the    DET     DT    67   87       pobj  \n","7          O     learn   VERB    VBG    89   99        acl  \n","8          O    -PRON-    DET   PRP$   101  120       pobj  \n","9          O       and  CCONJ     CC   122  124         cc  \n","10         O    -PRON-   PRON    PRP   126  127      nsubj  \n","11         O      find   VERB    VBP   129  132       conj  \n","12         O         a    DET     DT   134  164       dobj  \n","13         O        to   PART     TO   166  170      advcl  \n","14         O  superior    ADJ     JJ   172  179      acomp  \n","15         O        in    ADP     IN   181  182       prep  \n","16         O   several    ADJ     JJ   184  198       pobj  \n","17         O         .  PUNCT      .   199  199      punct  "],"text/html":["\n","  <div id=\"df-cb63664e-e122-44c6-928f-89c6419fd493\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Document#</th>\n","      <th>Sentence#</th>\n","      <th>Word#</th>\n","      <th>Word</th>\n","      <th>EntityType</th>\n","      <th>EntityIOB</th>\n","      <th>Lemma</th>\n","      <th>POS</th>\n","      <th>POSTag</th>\n","      <th>Start</th>\n","      <th>End</th>\n","      <th>Dependency</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>We</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>-PRON-</td>\n","      <td>PRON</td>\n","      <td>PRP</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>nsubj</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>compare</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>compare</td>\n","      <td>VERB</td>\n","      <td>VBP</td>\n","      <td>3</td>\n","      <td>9</td>\n","      <td>ROOT</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>a standard approach</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>a</td>\n","      <td>DET</td>\n","      <td>DT</td>\n","      <td>11</td>\n","      <td>29</td>\n","      <td>dobj</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>to infer</td>\n","      <td>VERB</td>\n","      <td>O</td>\n","      <td>to</td>\n","      <td>PART</td>\n","      <td>TO</td>\n","      <td>31</td>\n","      <td>38</td>\n","      <td>advcl</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>the sparse interactions</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>the</td>\n","      <td>DET</td>\n","      <td>DT</td>\n","      <td>40</td>\n","      <td>62</td>\n","      <td>dobj</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>in</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>in</td>\n","      <td>ADP</td>\n","      <td>IN</td>\n","      <td>64</td>\n","      <td>65</td>\n","      <td>prep</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>the Boltzmann machine</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>the</td>\n","      <td>DET</td>\n","      <td>DT</td>\n","      <td>67</td>\n","      <td>87</td>\n","      <td>pobj</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>learning to</td>\n","      <td>VERB</td>\n","      <td>O</td>\n","      <td>learn</td>\n","      <td>VERB</td>\n","      <td>VBG</td>\n","      <td>89</td>\n","      <td>99</td>\n","      <td>acl</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>our greedy algorithm</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>-PRON-</td>\n","      <td>DET</td>\n","      <td>PRP$</td>\n","      <td>101</td>\n","      <td>120</td>\n","      <td>pobj</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>and</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>and</td>\n","      <td>CCONJ</td>\n","      <td>CC</td>\n","      <td>122</td>\n","      <td>124</td>\n","      <td>cc</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>we</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>-PRON-</td>\n","      <td>PRON</td>\n","      <td>PRP</td>\n","      <td>126</td>\n","      <td>127</td>\n","      <td>nsubj</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>11</td>\n","      <td>find</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>find</td>\n","      <td>VERB</td>\n","      <td>VBP</td>\n","      <td>129</td>\n","      <td>132</td>\n","      <td>conj</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>12</td>\n","      <td>a greedy algorithm of inference</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>a</td>\n","      <td>DET</td>\n","      <td>DT</td>\n","      <td>134</td>\n","      <td>164</td>\n","      <td>dobj</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>to be</td>\n","      <td>VERB</td>\n","      <td>O</td>\n","      <td>to</td>\n","      <td>PART</td>\n","      <td>TO</td>\n","      <td>166</td>\n","      <td>170</td>\n","      <td>advcl</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>14</td>\n","      <td>superior</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>superior</td>\n","      <td>ADJ</td>\n","      <td>JJ</td>\n","      <td>172</td>\n","      <td>179</td>\n","      <td>acomp</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>15</td>\n","      <td>in</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>in</td>\n","      <td>ADP</td>\n","      <td>IN</td>\n","      <td>181</td>\n","      <td>182</td>\n","      <td>prep</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>several aspects</td>\n","      <td>ENTITY</td>\n","      <td>O</td>\n","      <td>several</td>\n","      <td>ADJ</td>\n","      <td>JJ</td>\n","      <td>184</td>\n","      <td>198</td>\n","      <td>pobj</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>17</td>\n","      <td>.</td>\n","      <td></td>\n","      <td>O</td>\n","      <td>.</td>\n","      <td>PUNCT</td>\n","      <td>.</td>\n","      <td>199</td>\n","      <td>199</td>\n","      <td>punct</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb63664e-e122-44c6-928f-89c6419fd493')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-cb63664e-e122-44c6-928f-89c6419fd493 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cb63664e-e122-44c6-928f-89c6419fd493');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":[""],"metadata":{"id":"M-YHQSMpRH_j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"TAvd91dsRICJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"st2D740zRIEv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"L5_Y7xnqRIGz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"NhjZwBxo9iUR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"C68kyygC9iWv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"fMqAwGDL9iZZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"PZ-xtrJ79icZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### ==================================================================================================\n","## Get more using Network shortest_paths\n","\n","\n","def get_paths(doc_triples):\n","    triples = []\n","    g, paths = get_paths_between_capitalised_entities(doc_triples)\n","    for p in paths:\n","        path = [(u, g[u][v]['key'], v) for (u, v) in zip(p[0:], p[1:])]\n","        length = len(p)\n","        if (path[length-2][1] == 'in' or path[length-2][1] == 'at' or path[length-2][1] == 'on'):\n","            if [path[0][0], path[length-2][1], path[length-2][2]] not in triples:\n","                triples.append([path[0][0], path[length-2][1], path[length-2][2]])\n","        elif (' in' in path[length-2][1] or ' at' in path[length-2][1] or ' on' in path[length-2][1]):\n","            if [path[0][0], path[length-2][1], path[length-2][2]] not in triples:\n","                triples.append([path[0][0], 'in', path[length-2][2]])\n","    for t in doc_triples:\n","        if t not in triples:\n","            triples.append(t)\n","    return triples\n","\n","def get_center(nodes):\n","    center = ''\n","    if (len(nodes) == 1):\n","        center = nodes[0]\n","    else:   \n","        # Capital letters and longer is preferred\n","        cap_ents = [e for e in nodes if any(x.isupper() for x in e)]\n","        if (cap_ents):\n","            center = max(cap_ents, key=len)\n","        else:\n","            center = max(nodes, key=len)\n","    return center\n","\n","def connect_graphs(mytriples):\n","    G = nx.DiGraph()\n","    for s, p, o in mytriples:\n","        G.add_edge(s, o, p=p)        \n","    \n","    \"\"\"\n","    # Get components\n","    graphs = list(nx.connected_component_subgraphs(G.to_undirected()))\n","    \n","    # Get the largest component\n","    largest_g = max(graphs, key=len)\n","    largest_graph_center = ''\n","    largest_graph_center = get_center(nx.center(largest_g))\n","    \n","    # for each graph, find the centre node\n","    smaller_graph_centers = []\n","    for g in graphs:        \n","        center = get_center(nx.center(g))\n","        smaller_graph_centers.append(center)\n","\n","    for n in smaller_graph_centers:\n","        if (largest_graph_center is not n):\n","            G.add_edge(largest_graph_center, n, p='with')\n","    \"\"\"\n","    return G\n","        \n","def rank_by_degree(mytriples): #, limit):\n","    G = connect_graphs(mytriples)\n","    degree_dict = dict(G.degree(G.nodes()))\n","    nx.set_node_attributes(G, degree_dict, 'degree')\n","    \n","    # Use this to draw the graph\n","    #draw_graph_centrality(G, degree_dict)\n","\n","    Egos = nx.DiGraph()\n","    for a, data in sorted(G.nodes(data=True), key=lambda x: x[1]['degree'], reverse=True):\n","        ego = nx.ego_graph(G, a)\n","        Egos.add_edges_from(ego.edges(data=True))\n","        Egos.add_nodes_from(ego.nodes(data=True))\n","        \n","        #if (nx.number_of_edges(Egos) > 20):\n","        #    break\n","       \n","    ranked_triples = []\n","    for u, v, d in Egos.edges(data=True):\n","        ranked_triples.append([u, d['p'], v])\n","    return ranked_triples\n","\n","\n","def draw_graph_centrality(G, dictionary):\n","    # plt.figure(figsize=(12,10))\n","    # pos = nx.spring_layout(G)\n","    # #print(\"Nodes\\n\", G.nodes(True))\n","    # #print(\"Edges\\n\", G.edges())\n","    \n","    # nx.draw_networkx_nodes(G, pos, \n","    #         nodelist=dictionary.keys(),\n","    #         with_labels=False,\n","    #         edge_color='black',\n","    #         width=1,\n","    #         linewidths=1,\n","    #         node_size = [v * 150 for v in dictionary.values()],\n","    #         node_color='blue',\n","    #         alpha=0.5)\n","    # edge_labels = {(u, v): d[\"p\"] for u, v, d in G.edges(data=True)}\n","    # #print(edge_labels)\n","    # nx.draw_networkx_edge_labels(G, pos,\n","    #                        font_size=10,\n","    #                        edge_labels=edge_labels,\n","    #                        font_color='blue')\n","    # nx.draw(G, pos, with_labels=True, node_size=1, node_color='blue')\n","    pass"],"metadata":{"id":"dHqQARM3N6A0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"iQwHQpeCN6Dw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"dleM6gQPN6GV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"qDuriAidN6JJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"KtA81ZYqN6LZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Rmz7H_NEN6OE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"NKqi8qnkN6Qw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"nfyP1uo6N6T7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","sentence ='[ARG0: Bob] agreed to (V: take) out (ARG1: the trash)'\n","#result_list = re.findall(r\"[[](.*?)[]]\", sentence)\n","\n","#print(result_list [0])\n"],"metadata":{"id":"0JmFyMpmN6WN","executionInfo":{"status":"ok","timestamp":1659060948845,"user_tz":-480,"elapsed":904,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["sentence[11:]"],"metadata":{"id":"GzEs658HN6Y2","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1659060259481,"user_tz":-480,"elapsed":16,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"}},"outputId":"cce1f0c4-0d3b-4ae3-bccd-43715d497232"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' agreed to [V: take] out (ARG1: the trash)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["re.findall(r\"[(](.*?)[)]\", sentence)"],"metadata":{"id":"Io5XUsagN6bq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659060366507,"user_tz":-480,"elapsed":436,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"}},"outputId":"cb80d63e-741a-43a1-a341-efdec8f5bfaa"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['ARG0: Bob', 'V: take', 'ARG1: the trash']"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["re.sub('\\[','(',sentence)"],"metadata":{"id":"ngykoKUZN6et","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1659060968929,"user_tz":-480,"elapsed":397,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"}},"outputId":"9b4496aa-7897-4fde-fc15-546d540ff6e3"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'(ARG0: Bob] agreed to (V: take) out (ARG1: the trash)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":[""],"metadata":{"id":"3rvDnzXFN6ha"},"execution_count":null,"outputs":[]}]}